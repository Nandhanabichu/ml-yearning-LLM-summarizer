# ML-yearning-LLM-summarizer
Fine-tuning an open-source LLM to summarize chapters from Machine Learning Yearning using LoRA

# LLM Fine-Tuning for Educational Textbook Summarization

This project implements an end-to-end pipeline to summarize chapters from the book *Machine Learning Yearning* using open-source transformer models and parameter-efficient fine-tuning techniques.

## üìå Project Overview
- Extracts text from a textbook PDF
- Splits long chapters into token-safe chunks
- Generates chapter-level summaries using transformer models
- Prepares instruction-style data for fine-tuning
- Fine-tunes an open-source LLM using LoRA

## üß† Models & Techniques
- Summarization: BART / DistilBART
- Fine-tuning: Falcon (LoRA)
- Instruction tuning for chapter summarization

## üõ†Ô∏è Tech Stack
- Python
- Hugging Face Transformers & Datasets
- PyTorch
- PEFT (LoRA)
- PyPDF2

## ‚ñ∂Ô∏è How to Run
1. Install dependencies:
   ```bash
   pip install -r requirements.txt
2. Upload the Machine Learning Yearning PDF (not included due to copyright)
   
3. Run the notebook ml_yearning_llm_finetuning.ipynb
