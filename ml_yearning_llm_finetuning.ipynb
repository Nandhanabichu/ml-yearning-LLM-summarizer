{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Fine-tune an Open-source LLM on a custom PDF**#"
      ],
      "metadata": {
        "id": "-LwyIsmKxvRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Installing dependencies###"
      ],
      "metadata": {
        "id": "ktgnSwrDyOM9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyBzTn-ac27u"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet --upgrade \"transformers>=4.34.0\" \"huggingface_hub>=0.18.0\" accelerate bitsandbytes peft datasets sentencepiece pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "YY9jb6LFi0No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "hlBsV0cQi5q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Upload and Extract PDF###"
      ],
      "metadata": {
        "id": "dt-OPYgoyaoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "print(\"Upload your Machine Learning Yearning PDF now (drag & drop)...\")\n",
        "uploaded = files.upload()\n",
        "pdf_path = list(uploaded.keys())[0]\n",
        "print(\"Uploaded file:\", pdf_path)"
      ],
      "metadata": {
        "id": "u8WH957ed2Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import PyPDF2\n",
        "\n",
        "summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    model=\"facebook/bart-large-cnn\",\n",
        "    device= 0\n",
        ")\n",
        "\n",
        "# Function to read PDF and extract text per page\n",
        "def read_pdf(file_path):\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        pdf = PyPDF2.PdfReader(f)\n",
        "        text = [page.extract_text() for page in pdf.pages]\n",
        "    return text\n",
        "\n",
        "# Function to split text into safe chunks under model token limit\n",
        "def chunk_text(text, max_chars=1800):\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    for paragraph in text.split(\"\\n\"):\n",
        "        if len(current_chunk) + len(paragraph) <= max_chars:\n",
        "            current_chunk += \" \" + paragraph\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = paragraph\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "# Summarize per chapter/page\n",
        "def summarize_pdf(pdf_path):\n",
        "    pages = read_pdf(pdf_path)\n",
        "    summaries = []\n",
        "    for i, page_text in enumerate(pages):\n",
        "        if not page_text or len(page_text.strip()) == 0:\n",
        "            continue\n",
        "        chunks = chunk_text(page_text)\n",
        "        piece_summaries = []\n",
        "        for chunk in chunks:\n",
        "            try:\n",
        "                s = summarizer(\n",
        "                    chunk,\n",
        "                    max_length=70,\n",
        "                    min_length=30,\n",
        "                    do_sample=False,\n",
        "                    truncation=True  # ensures safety\n",
        "                )[0][\"summary_text\"]\n",
        "                piece_summaries.append(s)\n",
        "            except Exception as e:\n",
        "                print(f\"Error summarizing chunk on page {i+1}: {e}\")\n",
        "        chapter_summary = \" \".join(piece_summaries)\n",
        "        summaries.append(f\"Chapter {i+1} Summary:\\n{chapter_summary}\\n\")\n",
        "    return summaries\n",
        "\n",
        "# Run summarization\n",
        "pdf_path = \"/content/Machine Learning Yearning.pdf\"\n",
        "chapter_summaries = summarize_pdf(pdf_path)\n",
        "\n",
        "# Save output\n",
        "with open(\"ml_yearning_summaries.txt\", \"w\") as f:\n",
        "    for summary in chapter_summaries:\n",
        "        f.write(summary + \"\\n\")\n",
        "\n",
        "print(\"Summarization complete. Saved to ml_yearning_summaries.txt\")\n"
      ],
      "metadata": {
        "id": "9jnH4Tm2im34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, json\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "def extract_pdf_text(path):\n",
        "    doc = fitz.open(path)\n",
        "    pages = []\n",
        "    for i in range(len(doc)):\n",
        "        text = doc[i].get_text(\"text\") or \"\"\n",
        "        pages.append(text)\n",
        "    return \"\\n\\n\".join(pages)\n",
        "\n",
        "raw_text = extract_pdf_text(pdf_path)\n",
        "# Normalize newlines\n",
        "raw_text = re.sub(r\"\\r\\n\", \"\\n\", raw_text)\n",
        "raw_text = re.sub(r\"\\n{3,}\", \"\\n\\n\", raw_text)\n",
        "\n",
        "chapter_regex = r'(?i)(?=(^Chapter\\s+\\d+\\b))'\n",
        "parts = re.split(chapter_regex, raw_text, flags=re.MULTILINE)\n",
        "\n",
        "chapters = []\n",
        "i = 0\n",
        "\n",
        "while i < len(parts):\n",
        "    part = parts[i].strip()\n",
        "    if re.match(r'(?i)^Chapter\\s+\\d+\\b', part):\n",
        "        title = part\n",
        "        content = parts[i+1] if i+1 < len(parts) else \"\"\n",
        "        if content.strip():\n",
        "            chapters.append({\"title\": title, \"content\": content.strip()})\n",
        "        i += 2\n",
        "    else:\n",
        "        # treat any leading text (preface/introduction) as Preface if non-empty\n",
        "        if part:\n",
        "            chapters.insert(0, {\"title\": \"Preface\", \"content\": part})\n",
        "        i += 1\n",
        "\n",
        "if len(chapters) == 0:\n",
        "    chapters = [{\"title\":\"Document\", \"content\": raw_text}]\n",
        "\n",
        "print(f\"Detected {len(chapters)} chapter(s). Example title: {chapters[0]['title']}\")"
      ],
      "metadata": {
        "id": "ffkbzyktd2MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_jsonl = \"mly_finetune.jsonl\"\n",
        "with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
        "    for ch in chapters:\n",
        "        ex = {\n",
        "            \"instruction\": f\"Summarize the following chapter ({ch['title']}) from the textbook in detail, covering key concepts and important points.\",\n",
        "            \"input\": ch[\"content\"],\n",
        "            \"output\": ch.get(\"summary\", \"\")\n",
        "        }\n",
        "        f.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n",
        "print(\"Saved\", out_jsonl, \"with\", len(chapters), \"examples.\")"
      ],
      "metadata": {
        "id": "EOdDhfxyd2FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load Dataset###"
      ],
      "metadata": {
        "id": "6qgnyu2zylQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"json\", data_files=out_jsonl, split=\"train\")\n",
        "print(\"Dataset loaded, examples:\", len(ds))"
      ],
      "metadata": {
        "id": "_JAYud92d18E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokenizer###"
      ],
      "metadata": {
        "id": "RHBrnv1Syt3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "def load_base_model_or_fallback(model_id):\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "        )\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=bnb_config,\n",
        "            trust_remote_code=False,\n",
        "        )\n",
        "        return tokenizer, model\n",
        "    except Exception as e:\n",
        "        print(\"Failed to load\", model_id, \"->\", str(e))\n",
        "        return None, None\n",
        "\n",
        "primary_model = \"tiiuae/falcon-7b-instruct\"\n",
        "tokenizer, model = load_base_model_or_fallback(primary_model)\n",
        "\n",
        "if model is None:\n",
        "\n",
        "    fallback_model = \"openlm-research/open_llama_3b_v2\"\n",
        "    print(\"Falling back to\", fallback_model)\n",
        "    tokenizer, model = load_base_model_or_fallback(fallback_model)\n",
        "    if model is None:\n",
        "        raise RuntimeError(\"Couldn't load primary or fallback model. Check internet/availability or pick another model.\")\n",
        "\n",
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n"
      ],
      "metadata": {
        "id": "xuv5VSlZd1sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "Ryye7X3VfQJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 1024\n",
        "pad_id = tokenizer.pad_token_id\n",
        "\n",
        "def build_prompt(instruction, inp):\n",
        "    return f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n\"\n",
        "\n",
        "def tokenize_and_make_labels(example):\n",
        "    # Build prompt and response separately to create labels that ignore prompt\n",
        "    prompt = build_prompt(example[\"instruction\"], example[\"input\"])\n",
        "    response = example[\"output\"]\n",
        "    # Encode without special tokens to control concatenation\n",
        "    p_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
        "    r_ids = tokenizer.encode(response, add_special_tokens=False)\n",
        "    input_ids = (p_ids + r_ids)[:max_length]\n",
        "    # Create labels: -100 for prompt tokens, response ids for response region\n",
        "    labels = [-100] * len(p_ids) + r_ids\n",
        "    labels = labels[:max_length]\n",
        "    # Pad if needed\n",
        "    if len(input_ids) < max_length:\n",
        "        pad_len = max_length - len(input_ids)\n",
        "        input_ids = input_ids + [pad_id] * pad_len\n",
        "        labels = labels + [-100] * pad_len\n",
        "    attention_mask = [1 if id != pad_id else 0 for id in input_ids]\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "# Map across dataset (batched=False for clarity; adjust if you hit speed issues)\n",
        "tokenized = ds.map(lambda ex: tokenize_and_make_labels(ex), remove_columns=ds.column_names)\n"
      ],
      "metadata": {
        "id": "eEjopuB4fQGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training arguements###"
      ],
      "metadata": {
        "id": "VmYWLykoy7fJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "def collate_fn(batch):\n",
        "    import torch\n",
        "    input_ids = torch.tensor([b[\"input_ids\"] for b in batch], dtype=torch.long)\n",
        "    attention_mask = torch.tensor([b[\"attention_mask\"] for b in batch], dtype=torch.long)\n",
        "    labels = torch.tensor([b[\"labels\"] for b in batch], dtype=torch.long)\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n"
      ],
      "metadata": {
        "id": "epIsxIhrfs9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mly_qlora_out\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    remove_unused_columns=False,\n",
        "    optim=\"paged_adamw_32bit\"\n",
        ")"
      ],
      "metadata": {
        "id": "CNMnUzgwfzn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Trainer###"
      ],
      "metadata": {
        "id": "GrEWAJu2xinp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized,\n",
        "    data_collator=collate_fn,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"Starting training... (this will use QLoRA adapters and is GPU heavy)\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "gQ_d5Biufzka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adapter_dir = \"./mly_finetuned_adapter\"\n",
        "model.save_pretrained(adapter_dir)\n",
        "tokenizer.save_pretrained(adapter_dir)\n",
        "print(\"Saved adapter + tokenizer to\", adapter_dir)\n"
      ],
      "metadata": {
        "id": "EdhnRT1Zfzh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evaluation###"
      ],
      "metadata": {
        "id": "WCPypZVlwv_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def generate_summary(chapter_text, max_new_tokens=250):\n",
        "    prompt = f\"### Instruction:\\nSummarize the following chapter from Machine Learning Yearning in detail:\\n\\n### Input:\\n{chapter_text}\\n\\n### Response:\\n\"\n",
        "    gen = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "    out = gen(prompt, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "    text = out[0][\"generated_text\"]\n",
        "    return text.split(\"### Response:\")[-1].strip()"
      ],
      "metadata": {
        "id": "SL3qEu1Qf902"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "kZE6fo8jak5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "oXErOQzyasD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "\n",
        "summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load ROUGE metric\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "# Example small manual evaluation set\n",
        "data = pd.DataFrame({\n",
        "    \"text\": [\n",
        "        \"Deep learning is a subset of machine learning that uses neural networks.\",\n",
        "        \"Reinforcement learning trains an agent to take actions to maximize rewards.\"\n",
        "    ],\n",
        "    \"reference_summary\": [\n",
        "        \"Deep learning is a type of ML using neural networks.\",\n",
        "        \"Reinforcement learning optimizes actions for rewards.\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "pred_summaries = []\n",
        "start_time = time.time()\n",
        "\n",
        "for doc in data[\"text\"]:\n",
        "    summary = summarizer(\n",
        "        doc,\n",
        "        max_length=50,\n",
        "        min_length=10,\n",
        "        do_sample=False,\n",
        "        truncation=True\n",
        "    )[0][\"summary_text\"]\n",
        "    pred_summaries.append(summary)\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "# Compute ROUGE\n",
        "results = rouge.compute(\n",
        "    predictions=pred_summaries,\n",
        "    references=data[\"reference_summary\"]\n",
        ")\n",
        "\n",
        "# Display results\n",
        "print(\"ðŸ“Š Evaluation Results:\")\n",
        "for k, v in results.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "print(f\"\\nâ± Processed {len(data)} samples in {elapsed:.2f} seconds (~{elapsed/len(data):.2f} sec/sample)\")\n",
        "\n",
        "# ðŸ“ Note\n",
        "print(\"\\nNOTE: The above scores are based on a very small manual test set for demonstration. \"\n",
        "      \"If the official evaluation dataset lacks valid summaries, ROUGE will be close to zero. \"\n",
        "      \"The purpose here is to verify that the fine-tuned model pipeline runs end-to-end.\")\n"
      ],
      "metadata": {
        "id": "THY6B0T1pISL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Show one example summary from the fine-tuned model\n",
        "sample_text = data[\"text\"].iloc[0]\n",
        "reference_summary = data[\"reference_summary\"].iloc[0]\n",
        "\n",
        "# Generate summary\n",
        "generated_summary = summarizer(\n",
        "    sample_text,\n",
        "    max_length=50,\n",
        "    min_length=10,\n",
        "    do_sample=False,\n",
        "    truncation=True\n",
        ")[0][\"summary_text\"]\n",
        "\n",
        "print(\"=== Example Summary ===\")\n",
        "print(\"\\nðŸ“œ Original Text:\\n\", sample_text)\n",
        "print(\"\\nâœ… Reference Summary:\\n\", reference_summary)\n",
        "print(\"\\nðŸ¤– Generated Summary:\\n\", generated_summary)\n"
      ],
      "metadata": {
        "id": "VKzDMWHRupRE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}